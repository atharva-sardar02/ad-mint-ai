<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>9</epicId>
    <storyId>2</storyId>
    <title>Video Generation Feedback Loop CLI</title>
    <status>drafted</status>
    <generatedAt>2025-11-17T21:30:00Z</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/9-2-video-generation-feedback-loop-cli.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer</asA>
    <iWant>a CLI tool for generating videos with automatic VBench quality scoring</iWant>
    <soThat>I can rapidly iterate on video generation and select the best candidates</soThat>
    <tasks>
- Task 1: Create video generation CLI service (AC: #1, #4, #5)
  - Create `app/services/pipeline/video_generation_cli.py` service
  - Implement text-to-video generation:
    - Call Replicate API with text-to-video model (Kling, Wan, PixVerse, or similar)
    - Support multiple video attempts (default: 3, configurable)
    - Download and save generated videos
    - Track generation metadata (prompt, model, settings, timestamps, cost)
  - Implement image-to-video generation:
    - Load and validate hero frame image
    - Call Replicate image-to-video API with hero frame + motion prompt
    - Support negative prompts for unwanted motion
    - Generate multiple video attempts with same inputs
    - Track hero frame path in metadata
  - Implement storyboard mode:
    - Load `storyboard_enhanced_motion_prompts.json` from Story 9.1
    - Parse enhanced motion prompts per clip
    - Extract start/end frame paths from JSON (preserved from Story 9.1)
    - For each clip:
      - Load start frame image (`start_frame_path`) and end frame image (`end_frame_path`)
      - Use enhanced motion prompt for that clip
      - Generate video using image-to-video model (start frame + motion prompt, or end frame as reference)
      - Track clip number and frame paths in metadata
    - Organize outputs by clip: `clip_001_video_001.mp4`, etc.
    - Save `storyboard_video_generation_trace.json` with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference
  - Implement video download and file organization:
    - Download videos from Replicate API URLs
    - Save videos to organized directory structure
    - Handle download errors and retries
  - Create `VideoGenerationResult` class:
    - videos: List[VideoMetadata] (sorted by quality score)
    - generation_trace: Dict (complete trace with prompts, videos, scores, costs, timestamps)
    - mode: str ("text-to-video" or "image-to-video" or "storyboard")
    - hero_frame_path: Optional[str] (if image-to-video mode)
    - storyboard_ref: Optional[str] (if storyboard mode)
  - Create `VideoMetadata` class:
    - file_path: str
    - rank: int
    - vbench_scores: Dict[str, float] (all 16 VBench dimensions + overall)
    - generation_params: Dict (prompt, model, settings, seed, timestamp, cost)
  - Create `StoryboardVideoGenerationResult` class:
    - storyboard_path: Path to storyboard_enhanced_motion_prompts.json
    - clips: List[Dict] with videos per clip
    - clip_results: List[VideoGenerationResult] for each clip
    - clip_frame_paths: Dict mapping clip numbers to start/end frame paths (preserved from Story 9.1)
    - unified_narrative_path: Optional[str] (preserved from Story 9.1)
    - summary: Dict with overall statistics
  - Unit tests: Replicate API integration (mocked), video download, file organization, metadata generation, text-to-video mode, image-to-video mode, storyboard mode, error handling

- Task 2: Integrate VBench quality scoring (AC: #2)
  - Create `app/services/pipeline/video_quality_scoring.py` wrapper service
  - Integrate with existing `quality_control.py` VBench evaluation (Story 7.6):
    - Call `evaluate_vbench()` function from `quality_control.py`
    - Pass video file path and prompt text
    - Receive VBench scores dictionary with all 16 dimensions
  - Implement overall quality score calculation:
    - Weighted combination of VBench dimensions
    - Temporal Quality: 40% (subject_consistency, background_consistency, motion_smoothness, dynamic_degree)
    - Frame-wise Quality: 35% (aesthetic_quality, imaging_quality, object_class_alignment)
    - Text-Video Alignment: 25% (text_video_alignment)
    - Calculate overall_quality score (0-100)
  - Implement video ranking:
    - Rank videos by overall quality score (best first)
    - Sort and rename videos by rank (video_001 = best, video_002 = second-best, etc.)
  - Support batch scoring for multiple video attempts
  - Handle VBench library unavailability:
    - Use fallback metrics from `quality_control.py` if VBench library unavailable
    - Continue processing even if some videos fail to score
    - Report which videos couldn't be scored
  - Unit tests: VBench integration wrapper, verify calls to quality_control.py, test overall score calculation, test ranking logic, test fallback metrics

- Task 3: Create CLI tool for video generation (AC: #3, #6, #7)
  - Create `backend/generate_videos.py` CLI script
  - Implement argument parsing (argparse or click):
    - Input: enhanced prompt file path (for text-to-video) OR storyboard_enhanced_motion_prompts.json (for storyboard mode)
    - `--image-to-video` (optional flag, enable image-to-video mode)
    - `--hero-frame PATH` (required if --image-to-video, path to hero frame image)
    - `--motion-prompt S` (required if --image-to-video, motion prompt text)
    - `--storyboard PATH` (optional, path to storyboard_enhanced_motion_prompts.json from Story 9.1)
    - `--num-attempts N` (default: 3, number of video attempts)
    - `--negative-prompt S` (optional, negative prompt text)
    - `--output-dir DIR` (default: output/video_generations/)
    - `--model M` (default: kling/kling-v1, Replicate model name)
    - `--interactive` (optional flag, enable human-in-the-loop feedback)
    - `--verbose` flag
  - Implement input validation:
    - Validate prompt file exists (if text-to-video mode)
    - Validate hero frame exists (if image-to-video mode)
    - Validate storyboard JSON exists and has correct structure (if storyboard mode)
    - Validate start/end frame images exist (if storyboard mode)
    - Validate mutually exclusive modes (text-to-video vs image-to-video vs storyboard)
  - Implement prompt file loading (for text-to-video mode)
  - Implement storyboard JSON loading:
    - Load and parse `storyboard_enhanced_motion_prompts.json`
    - Validate JSON structure (clips array, enhanced motion prompts, start_frame_path, end_frame_path per clip)
    - Extract enhanced motion prompts per clip
    - Extract start/end frame paths per clip
    - Extract unified narrative document reference (if available)
    - Validate all frame images exist and are readable
  - Implement output directory creation with timestamp
  - Integrate video generation service
  - Integrate VBench quality scoring service
  - Implement video ranking and renaming:
    - Rank videos by overall quality score
    - Rename videos by rank (video_001.mp4 = best, video_002.mp4 = second-best, etc.)
    - For storyboard mode: Organize by clip (clip_001_video_001.mp4, etc.)
  - Implement trace file saving:
    - Save metadata JSON for each video (VBench scores, prompt, model, settings)
    - Save `generation_trace.json` with complete trace (all prompts, videos, VBench scores, costs, timestamps, hero frame path if image-to-video, storyboard references if used)
    - For storyboard mode: Save `storyboard_video_generation_trace.json` with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference
  - Implement console output formatting:
    - Progress indicators for video generation and VBench scoring
    - Ranked list of videos with VBench scores
    - Top video highlighted as "system-selected best"
    - VBench score breakdown per video (key dimensions)
    - Comparison summary showing why top video scored highest
    - File paths for manual viewing
    - Cost summary
    - For storyboard mode: List of clips processed, best video per clip, score breakdown per clip
  - Implement human feedback support (if --interactive):
    - Pause after generation for manual review
    - Accept feedback via CLI prompts
    - Incorporate feedback into next iteration (if regenerating)
  - Unit tests: Argument parsing, file I/O, error handling, storyboard JSON parsing, validation logic
  - Integration test: End-to-end CLI run with enhanced prompt (text-to-video), verify videos generated, verify VBench scoring, verify ranking
  - Integration test: End-to-end CLI run with hero frame + motion prompt (image-to-video), verify image-to-video mode works, verify VBench scoring
  - Integration test: End-to-end CLI run with storyboard JSON, verify per-clip video generation, verify start/end frame usage, verify VBench scoring per clip, verify storyboard_video_generation_trace.json created

- Task 4: Documentation and testing (All ACs)
  - Update `backend/requirements.txt` with any new dependencies (replicate, requests, opencv-python if needed)
  - Create README or usage documentation for CLI tool
  - Add integration tests to `backend/tests/test_video_generation_cli.py`
  - Verify error handling for API failures, invalid inputs, missing files, VBench scoring failures, missing storyboard files, missing frame images
  - Performance test: Verify video generation completes within <10 minutes for 3 videos (target)
  - Performance test: Verify VBench scoring completes within <5 minutes for 3 videos (target)
  - Document VBench integration and fallback behavior
    </tasks>
  </story>

  <acceptanceCriteria>
1. **Given** I have:
   - An enhanced video prompt (from Story 9.1), OR
   - A hero frame image + motion prompt (from Epic 8), OR
   - Storyboard start/end frames with enhanced motion prompts (from Story 9.1 storyboard mode)
   **When** I run `python generate_videos.py enhanced_prompt.txt --num-attempts 3`  
   **Then** the CLI tool:
   - For text-to-video: Calls a text-to-video model (e.g., Kling/Wan/PixVerse on Replicate)
   - For image-to-video: Calls an image-to-video model with hero frame + motion prompt
   - For storyboard mode: Generates videos from storyboard start/end frames using enhanced motion prompts from Story 9.1
   - Generates multiple video attempts (default: 3, configurable)
   - All videos follow the enhanced prompt and settings

2. **Given** videos are generated
   **Then** the system automatically scores each video using VBench:
   - **Temporal Quality**:
     - Subject identity consistency (0-100)
     - Background consistency (0-100)
     - Motion smoothness (0-100)
     - Dynamic degree (0-100)
   - **Frame-wise Quality**:
     - Aesthetic quality (0-100)
     - Imaging quality (0-100)
     - Object class alignment (0-100)
   - **Text-Video Alignment**:
     - Prompt adherence (0-100)
   - **Overall Quality Score**: Weighted combination of all VBench dimensions

3. **Given** videos are generated and scored
   **Then** the CLI:
   - Saves all generated videos to `output/video_generations/{timestamp}/`:
     - `video_001.mp4`, `video_002.mp4`, etc. (numbered by quality rank)
     - `video_001_metadata.json`, `video_002_metadata.json` (VBench scores, prompt, model, settings)
   - Saves generation trace to `output/video_generations/{timestamp}/generation_trace.json`:
     - All prompts used (original, enhanced, motion, negative)
     - All videos generated with file paths
     - All VBench scores per video (all 16 dimensions)
     - Model settings, seeds, timestamps, costs
     - Hero frame path (if image-to-video)
     - Storyboard references (if used): storyboard_metadata.json path, enhanced motion prompts JSON path, start/end frame paths per clip
   - Prints results to console:
     - Ranked list of videos by overall quality score
     - Top video highlighted as "system-selected best"
     - VBench score breakdown per video (key dimensions)
     - File paths for manual viewing
     - Cost summary
   - Automatically selects the top-scoring video as "system-selected best"
   - Provides comparison summary showing why top video scored highest

4. **Given** I use image-to-video mode
   **When** I run `python generate_videos.py --image-to-video --hero-frame path/to/image.png --motion-prompt "camera pans left"`
   **Then** the CLI tool:
   - Loads hero frame image and validates it exists
   - Uses motion prompt for image-to-video generation
   - Calls Replicate image-to-video API (Kling, Wan, PixVerse, or similar)
   - Generates multiple video attempts with same hero frame + motion prompt
   - Scores each video using VBench
   - Saves hero frame path in generation trace

5. **Given** I use storyboard mode
   **When** I run `python generate_videos.py --storyboard path/to/storyboard_enhanced_motion_prompts.json`
   **Then** the CLI tool:
   - Loads `storyboard_enhanced_motion_prompts.json` from Story 9.1
   - Extracts enhanced motion prompts for each clip
   - Extracts start/end frame paths for each clip (from storyboard_enhanced_motion_prompts.json)
   - For each clip:
     - Loads start frame image (`start_frame_path`) and end frame image (`end_frame_path`)
     - Uses enhanced motion prompt for that clip
     - Generates video using image-to-video model with start frame + motion prompt (or end frame as reference)
     - Scores video using VBench
   - Saves all clip videos to organized structure:
     - `clip_001_video_001.mp4`, `clip_001_video_002.mp4`, etc. (per clip, ranked by quality)
     - `clip_001_video_001_metadata.json`, etc. (VBench scores per video per clip)
   - Saves `storyboard_video_generation_trace.json` with:
     - All clips' videos and scores
     - Start/end frame paths preserved
     - Enhanced motion prompts used
     - Unified narrative document reference (if available)
   - Prints results to console:
     - List of clips processed
     - Best video per clip (ranked by quality)
     - VBench score breakdown per clip
     - File paths for manual viewing

6. **Given** the CLI tool supports various options
   **Then** it:
   - Supports custom output directory: `--output-dir ./my_videos`
   - Supports image-to-video mode: `--image-to-video --hero-frame path/to/image.png --motion-prompt "camera pans left"`
   - Supports storyboard mode: `--storyboard path/to/storyboard_enhanced_motion_prompts.json`
   - Supports negative prompts: `--negative-prompt "jerky, flicker, inconsistent"`
   - Supports model selection: `--model kling/kling-v1` (default: kling/kling-v1 or similar)
   - Supports number of attempts: `--num-attempts 5` (default: 3)
   - Supports verbose logging: `--verbose`

7. **Given** the system supports human feedback
   **Then** it:
   - Logs all API calls and costs for transparency
   - Supports human feedback: After viewing videos manually, user can provide feedback via CLI for next iteration
   - Pauses for manual review (if `--interactive` flag enabled)
   - Accepts feedback via CLI prompts
   - Incorporates feedback into next iteration (if regenerating)
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/sprint-artifacts/tech-spec-epic-9.md</path>
        <title>Epic 9 Technical Specification</title>
        <section>Story 9.2: Video Generation Feedback Loop CLI</section>
        <snippet>Detailed design for video generation CLI service with VBench quality scoring integration. Includes acceptance criteria, traceability mapping, non-functional requirements (performance, security, reliability), and storyboard mode integration.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epics and Stories</title>
        <section>Story 9.2: Video Generation Feedback Loop CLI (FR-040)</section>
        <snippet>Original story acceptance criteria and prerequisites: Story 9.1 (Video Prompt Enhancement), Story 7.6 (VBench Integration), Story 3.2 (Video Generation Infrastructure). Technical notes on Replicate API integration and VBench reuse.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Hero-Frame Iteration Plan & Timeline (Section 23.0)</section>
        <snippet>CLI MVP approach for rapid iteration and validation before developing UI components. Outlines the feedback loop workflow: image prompt → image generation → video prompt → video generation.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Project Structure</section>
        <snippet>Project structure patterns, service organization in `app/services/pipeline/`, configuration management via `app/core/config.py`, and output directory structure.</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/9-1-video-prompt-feedback-loop-cli.md</path>
        <title>Story 9.1: Video Prompt Feedback Loop CLI</title>
        <section>Dev Notes</section>
        <snippet>Story 9.1 generates `storyboard_enhanced_motion_prompts.json` when processing storyboards. This JSON includes enhanced motion prompts per clip, start/end frame paths (preserved from storyboard), unified narrative document reference, and scores/iteration history per clip. Frame paths are explicitly preserved for use in Story 9.2 video generation.</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/8-2-image-generation-feedback-loop-cli.md</path>
        <title>Story 8.2: Image Generation Feedback Loop CLI</title>
        <section>Dev Notes</section>
        <snippet>CLI pattern for multiple generation attempts, automatic quality scoring and ranking, file renaming by quality rank, metadata JSON generation, console output formatting with scores, and cost tracking. Pattern to follow for Story 9.2.</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/8-3-storyboard-creation-cli.md</path>
        <title>Story 8.3: Storyboard Creation CLI</title>
        <section>Dev Notes</section>
        <snippet>Storyboard service generates `storyboard_metadata.json` with clips array. Each clip includes `start_frame_path` and `end_frame_path` pointing to PNG files. Story 9.1 processes this and outputs `storyboard_enhanced_motion_prompts.json` which preserves these frame paths for Story 9.2.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>backend/app/services/pipeline/video_generation.py</path>
        <kind>service</kind>
        <symbol>generate_video_clip_with_model()</symbol>
        <lines>81-163</lines>
        <reason>Replicate API integration pattern for video generation. Function signature: `async def generate_video_clip_with_model(prompt: str, duration: int, output_dir: str, generation_id: str, model_name: str, ...) -> tuple[str, str]`. Handles model selection, retry logic with exponential backoff, cost tracking, seed parameter support, and video download. Reuse this pattern for Story 9.2 video generation service.</reason>
      </artifact>
      <artifact>
        <path>backend/app/services/pipeline/quality_control.py</path>
        <kind>service</kind>
        <symbol>evaluate_vbench()</symbol>
        <lines>45-124</lines>
        <reason>**CRITICAL** - VBench quality evaluation function from Story 7.6. Function signature: `def evaluate_vbench(video_clip_path: str, prompt_text: str, vbench_available: bool = VBENCH_AVAILABLE) -> Dict[str, float]`. Returns VBench scores dictionary with all 16 dimensions (temporal_quality, subject_consistency, background_consistency, motion_smoothness, dynamic_degree, aesthetic_quality, imaging_quality, object_class_alignment, text_video_alignment, overall_quality). Currently uses fallback metrics if VBench library unavailable, but structure ready for VBench integration. Performance target: <30 seconds per clip evaluation. Story 9.2 MUST reuse this function.</reason>
      </artifact>
      <artifact>
        <path>backend/app/services/pipeline/image_generation.py</path>
        <kind>service</kind>
        <symbol>generate_images()</symbol>
        <lines>90-431</lines>
        <reason>Replicate API integration pattern for images. Multiple generation attempts handling, file organization and metadata generation, cost tracking. Adapt this pattern for Story 9.2 video generation service.</reason>
      </artifact>
      <artifact>
        <path>backend/app/services/pipeline/image_quality_scoring.py</path>
        <kind>service</kind>
        <symbol>rank_images_by_quality()</symbol>
        <lines>36-200</lines>
        <reason>Quality scoring service wrapper pattern. Ranking logic (sort by overall score), file renaming by rank. Adapt this pattern for Story 9.2 video quality scoring wrapper that calls `quality_control.py`.</reason>
      </artifact>
      <artifact>
        <path>backend/generate_images.py</path>
        <kind>cli</kind>
        <symbol>main()</symbol>
        <lines>121-643</lines>
        <reason>CLI tool pattern to follow for `generate_videos.py`. Argument parsing with argparse, multiple generation attempts, automatic quality scoring and ranking, file renaming by quality rank, metadata JSON generation, console output formatting with scores, cost tracking, verbose logging mode, custom output directories, stdin input handling.</reason>
      </artifact>
      <artifact>
        <path>backend/app/core/config.py</path>
        <kind>config</kind>
        <symbol>settings</symbol>
        <lines>1-100</lines>
        <reason>Configuration management for API keys. Access Replicate API token via `settings.REPLICATE_API_TOKEN` (for video generation). Follow existing pattern for environment variable loading.</reason>
      </artifact>
      <artifact>
        <path>backend/app/services/pipeline/storyboard_service.py</path>
        <kind>service</kind>
        <symbol>create_storyboard()</symbol>
        <lines>1-300</lines>
        <reason>Storyboard service generates storyboard with start/end frames. `StoryboardResult` class with clips list and metadata. `ClipStoryboard` class with motion_description, camera_movement, start/end frame paths. Storyboard metadata JSON structure: clips array with motion descriptions, camera metadata, frame paths. Start/end frame images generated as PNG files (`clip_XXX_start.png`, `clip_XXX_end.png`) for use in image-to-video generation (Story 9.2).</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="replicate" version=">=0.25.0">Replicate API client for video generation (text-to-video and image-to-video models)</package>
        <package name="openai" version=">=1.0.0">OpenAI API client (if needed for any LLM calls)</package>
        <package name="requests" version=">=2.31.0">HTTP requests for downloading videos from Replicate API URLs</package>
        <package name="opencv-python" version=">=4.8.0">Video processing for VBench evaluation (if fallback metrics used, already in quality_control.py)</package>
        <package name="click" version=">=8.1.0">CLI argument parsing (or use argparse from stdlib, as generate_images.py does)</package>
      </python>
      <system>
        <requirement>Python 3.11+ (matches existing backend requirements)</requirement>
        <requirement>Replicate API token (REPLICATE_API_TOKEN environment variable)</requirement>
        <requirement>VBench library (optional, fallback metrics available in quality_control.py if library unavailable)</requirement>
      </system>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Service Location: New service files must be created in `app/services/pipeline/` directory following existing service structure.</constraint>
    <constraint>CLI Tool Location: New CLI tool must be created in `backend/` directory as standalone script (not part of FastAPI app).</constraint>
    <constraint>Input Modes: CLI tool must support three mutually exclusive modes: text-to-video (prompt file), image-to-video (hero frame + motion prompt), storyboard mode (storyboard_enhanced_motion_prompts.json from Story 9.1).</constraint>
    <constraint>Storyboard Processing: When using storyboard mode, MUST load `storyboard_enhanced_motion_prompts.json` from Story 9.1 output. Extract enhanced motion prompts per clip, extract start/end frame paths per clip (preserved from Story 9.1), validate all frame images exist and are readable, use start frame + enhanced motion prompt for image-to-video generation per clip.</constraint>
    <constraint>Output Directory Structure: Follow existing pattern: `backend/output/video_generations/{timestamp}/` for video files and metadata. Timestamp format: `YYYYMMDD_HHMMSS` (e.g., `20250117_143022`). Numbered video files: `video_001.mp4`, `video_002.mp4`, etc. (numbered by quality rank). Metadata JSON files: `video_001_metadata.json`, etc. Generation trace JSON: `generation_trace.json`. For storyboard mode: `clip_001_video_001.mp4`, etc. (organized by clip).</constraint>
    <constraint>VBench Integration: **CRITICAL** - MUST reuse existing `evaluate_vbench()` function from `app/services/pipeline/quality_control.py` (Story 7.6). Do NOT recreate VBench evaluation logic. Create wrapper service `video_quality_scoring.py` that calls `quality_control.py`. Handle VBench library unavailability gracefully using fallback metrics from `quality_control.py`.</constraint>
    <constraint>Video Generation Service: Reuse existing `app/services/pipeline/video_generation.py` service pattern (Story 3.2). Use `generate_video_clip_with_model()` function for Replicate API integration. Adapt for CLI use case (multiple attempts, file organization, metadata tracking).</constraint>
    <constraint>Quality Score Calculation: Implement overall quality score as weighted combination: Temporal Quality 40% (subject_consistency, background_consistency, motion_smoothness, dynamic_degree), Frame-wise Quality 35% (aesthetic_quality, imaging_quality, object_class_alignment), Text-Video Alignment 25% (text_video_alignment). Calculate overall_quality score (0-100).</constraint>
    <constraint>Video Ranking: Rank videos by overall quality score (best first). Sort and rename videos by rank (video_001 = best, video_002 = second-best, etc.). For storyboard mode: Organize by clip (clip_001_video_001.mp4, etc.).</constraint>
    <constraint>Trace File Structure: Save comprehensive trace files (JSON metadata) for transparency. Include all prompts, videos, VBench scores (all 16 dimensions), costs, timestamps, hero frame path (if image-to-video), storyboard references (if used). For storyboard mode: Save `storyboard_video_generation_trace.json` with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference.</constraint>
    <constraint>Configuration Management: Use `app/core/config.py` for API keys. Access Replicate API token via `settings.REPLICATE_API_TOKEN`. Follow existing pattern for environment variable loading.</constraint>
    <constraint>Performance Target: Video generation should complete within <10 minutes for 3 videos (target). VBench quality scoring should complete within <5 minutes for 3 videos (target). Log timing information for each major operation (generation, VBench scoring).</constraint>
    <constraint>Error Handling: All CLI tools must handle API failures gracefully with clear error messages. Retry logic for transient failures (rate limits, network issues). If VBench scoring fails for some videos, tool should continue and report which videos couldn't be scored. If storyboard files or frame images missing, provide clear error messages.</constraint>
    <constraint>Storyboard JSON Structure: `storyboard_enhanced_motion_prompts.json` from Story 9.1 contains: `storyboard_path`, `unified_narrative_path`, `clips` array (each with `clip_number`, `enhanced_motion_prompt`, `start_frame_path`, `end_frame_path`, `scores`, `camera_movement`, etc.), `clip_frame_paths` dict mapping clip numbers to frame paths, `summary` dict. MUST preserve frame paths and unified narrative reference in output trace.</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>VideoGenerationResult</name>
      <kind>Python class</kind>
      <signature>class VideoGenerationResult:
    videos: List[VideoMetadata]  # sorted by quality score
    generation_trace: Dict  # complete trace with prompts, videos, scores, costs, timestamps
    mode: str  # "text-to-video" or "image-to-video" or "storyboard"
    hero_frame_path: Optional[str]  # if image-to-video mode
    storyboard_ref: Optional[str]  # if storyboard mode</signature>
      <path>app/services/pipeline/video_generation_cli.py</path>
    </interface>
    <interface>
      <name>VideoMetadata</name>
      <kind>Python class</kind>
      <signature>class VideoMetadata:
    file_path: str
    rank: int
    vbench_scores: Dict[str, float]  # all 16 VBench dimensions + overall
    generation_params: Dict  # prompt, model, settings, seed, timestamp, cost</signature>
      <path>app/services/pipeline/video_generation_cli.py</path>
    </interface>
    <interface>
      <name>StoryboardVideoGenerationResult</name>
      <kind>Python class</kind>
      <signature>class StoryboardVideoGenerationResult:
    storyboard_path: Path  # to storyboard_enhanced_motion_prompts.json
    clips: List[Dict]  # with videos per clip
    clip_results: List[VideoGenerationResult]  # for each clip
    clip_frame_paths: Dict  # mapping clip numbers to start/end frame paths (preserved from Story 9.1)
    unified_narrative_path: Optional[str]  # preserved from Story 9.1
    summary: Dict  # overall statistics</signature>
      <path>app/services/pipeline/video_generation_cli.py</path>
    </interface>
    <interface>
      <name>evaluate_vbench()</name>
      <kind>Python function</kind>
      <signature>def evaluate_vbench(
    video_clip_path: str,
    prompt_text: str,
    vbench_available: bool = VBENCH_AVAILABLE
) -> Dict[str, float]</signature>
      <path>app/services/pipeline/quality_control.py</path>
    </interface>
    <interface>
      <name>generate_video_clip_with_model()</name>
      <kind>Python async function</kind>
      <signature>async def generate_video_clip_with_model(
    prompt: str,
    duration: int,
    output_dir: str,
    generation_id: str,
    model_name: str,
    cancellation_check: Optional[callable] = None,
    clip_index: Optional[int] = None
) -> tuple[str, str]</signature>
      <path>app/services/pipeline/video_generation.py</path>
    </interface>
    <interface>
      <name>storyboard_enhanced_motion_prompts.json</name>
      <kind>JSON data format</kind>
      <signature>{
  "storyboard_path": "path/to/storyboard_metadata.json",
  "unified_narrative_path": "path/to/unified_narrative.md",
  "clips": [
    {
      "clip_number": 1,
      "enhanced_motion_prompt": "...",
      "start_frame_path": "output/storyboards/.../clip_001_start.png",
      "end_frame_path": "output/storyboards/.../clip_001_end.png",
      "scores": {...},
      "camera_movement": "...",
      ...
    }
  ],
  "clip_frame_paths": {
    "1": {
      "start_frame_path": "...",
      "end_frame_path": "..."
    }
  },
  "summary": {...}
}</signature>
      <path>Output from Story 9.1 (input to Story 9.2)</path>
    </interface>
  </interfaces>

  <tests>
    <standards>Use pytest framework (matches existing backend test structure). Target: >80% code coverage. Test video generation service: Replicate API integration (mocked), video download, file organization, metadata generation, text-to-video mode, image-to-video mode, storyboard mode, error handling. Test VBench quality scoring wrapper: Verify calls to quality_control.py, test overall score calculation, test ranking logic, test fallback metrics. Test CLI argument parsing, file I/O, error handling, storyboard JSON parsing, validation logic.</standards>
    <locations>Unit tests: `backend/tests/test_video_generation_cli.py`, `backend/tests/test_video_quality_scoring.py`. Integration tests: `backend/tests/integration/test_video_generation_cli_integration.py`.</locations>
    <ideas>
      <idea>Unit test: Replicate API integration (mocked) - verify `generate_video_clip_with_model()` called with correct parameters, verify video download from API URL, verify file saved to correct location, verify metadata tracked correctly, verify cost calculation.</idea>
      <idea>Unit test: Video download and file organization - verify videos downloaded from Replicate API URLs, verify files saved to organized directory structure, verify download errors and retries handled, verify file naming by rank.</idea>
      <idea>Unit test: VBench integration wrapper - verify `evaluate_vbench()` called from `quality_control.py` with correct parameters (video path, prompt text), verify VBench scores dictionary received, verify overall quality score calculation (weighted combination), verify ranking logic (sort by overall score), verify fallback metrics used if VBench library unavailable.</idea>
      <idea>Unit test: Text-to-video mode - verify prompt file loaded, verify Replicate API called with text-to-video model, verify multiple video attempts generated, verify all videos scored using VBench, verify videos ranked and renamed by quality.</idea>
      <idea>Unit test: Image-to-video mode - verify hero frame image loaded and validated, verify Replicate image-to-video API called with hero frame + motion prompt, verify negative prompts supported, verify multiple video attempts generated with same inputs, verify hero frame path tracked in metadata.</idea>
      <idea>Unit test: Storyboard mode - verify `storyboard_enhanced_motion_prompts.json` loaded and parsed, verify JSON structure validated (clips array, enhanced motion prompts, start_frame_path, end_frame_path per clip), verify enhanced motion prompts extracted per clip, verify start/end frame paths extracted per clip, verify unified narrative document reference extracted, verify all frame images validated (exist and readable), verify per-clip video generation (start frame + enhanced motion prompt), verify outputs organized by clip (clip_001_video_001.mp4, etc.), verify `storyboard_video_generation_trace.json` created with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference.</idea>
      <idea>Unit test: Video ranking and renaming - verify videos ranked by overall quality score (best first), verify videos renamed by rank (video_001.mp4 = best, video_002.mp4 = second-best, etc.), verify for storyboard mode: organized by clip (clip_001_video_001.mp4, etc.).</idea>
      <idea>Unit test: Trace file saving - verify metadata JSON saved for each video (VBench scores, prompt, model, settings), verify `generation_trace.json` saved with complete trace (all prompts, videos, VBench scores, costs, timestamps, hero frame path if image-to-video, storyboard references if used), verify for storyboard mode: `storyboard_video_generation_trace.json` saved with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference.</idea>
      <idea>Unit test: CLI argument parsing - verify all arguments parsed correctly (prompt file, --image-to-video, --hero-frame, --motion-prompt, --storyboard, --num-attempts, --negative-prompt, --output-dir, --model, --interactive, --verbose), verify mutually exclusive modes validated (text-to-video vs image-to-video vs storyboard), verify required arguments validated (hero-frame if --image-to-video, motion-prompt if --image-to-video).</idea>
      <idea>Unit test: Input validation - verify prompt file exists (if text-to-video mode), verify hero frame exists (if image-to-video mode), verify storyboard JSON exists and has correct structure (if storyboard mode), verify start/end frame images exist (if storyboard mode), verify mutually exclusive modes (text-to-video vs image-to-video vs storyboard).</idea>
      <idea>Unit test: Error handling - verify API failures handled gracefully with clear error messages, verify retry logic for transient failures (rate limits, network issues), verify if VBench scoring fails for some videos, tool continues and reports which videos couldn't be scored, verify if storyboard files or frame images missing, provide clear error messages.</idea>
      <idea>Integration test: End-to-end CLI run with enhanced prompt (text-to-video) - run CLI with enhanced video prompt file, verify videos generated (3 attempts default), verify VBench scoring called for each video, verify videos ranked by quality score, verify videos renamed by rank (video_001.mp4 = best), verify metadata JSON files created, verify `generation_trace.json` created with complete trace, verify console output shows ranked list with top video highlighted, verify cost summary printed.</idea>
      <idea>Integration test: End-to-end CLI run with hero frame + motion prompt (image-to-video) - run CLI with --image-to-video --hero-frame path/to/image.png --motion-prompt "camera pans left", verify hero frame loaded and validated, verify Replicate image-to-video API called, verify multiple video attempts generated, verify VBench scoring called for each video, verify hero frame path saved in generation trace, verify console output shows results.</idea>
      <idea>Integration test: End-to-end CLI run with storyboard JSON - run CLI with --storyboard path/to/storyboard_enhanced_motion_prompts.json, verify storyboard JSON loaded and parsed, verify enhanced motion prompts extracted per clip, verify start/end frame paths extracted per clip, verify all frame images validated, verify per-clip video generation (start frame + enhanced motion prompt), verify VBench scoring called per clip, verify outputs organized by clip (clip_001_video_001.mp4, etc.), verify `storyboard_video_generation_trace.json` created with all clips' videos, scores, frame paths, enhanced prompts, unified narrative reference, verify console output shows list of clips processed with best video per clip and score breakdown per clip.</idea>
      <idea>Performance test: Verify video generation completes within <10 minutes for 3 videos (target), verify VBench scoring completes within <5 minutes for 3 videos (target), log timing information for each major operation (generation, VBench scoring).</idea>
    </ideas>
  </tests>
</story-context>

