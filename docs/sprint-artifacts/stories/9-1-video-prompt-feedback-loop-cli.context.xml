<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>9</epicId>
    <storyId>1</storyId>
    <title>Video Prompt Feedback Loop CLI</title>
    <status>drafted</status>
    <generatedAt>2025-11-17T20:32:00Z</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/9-1-video-prompt-feedback-loop-cli.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer</asA>
    <iWant>a CLI tool for iterative video prompt enhancement using two-agent feedback loops with VideoDirectorGPT enhancements</iWant>
    <soThat>I can rapidly refine prompts for video generation without UI overhead</soThat>
    <tasks>
- Task 1: Create video prompt enhancement service (AC: #1, #2, #3, #4)
  - Create `app/services/pipeline/video_prompt_enhancement.py` service
  - Implement two-agent pattern adapting from `image_prompt_enhancement.py`:
    - Agent 1 (Video Director): Adapt system prompt for video-specific enhancement (motion, temporal coherence, cinematography focus)
    - Agent 2 (Prompt Engineer): Adapt scoring to 6 dimensions (add Temporal Coherence dimension)
    - Implement iterative refinement loop (max 3 rounds) with convergence detection
  - Implement scoring on 6 dimensions (0-100 each): Completeness, Specificity, Professionalism, Cinematography, Temporal Coherence, Brand Alignment
  - Implement VideoDirectorGPT planning integration (if Story 7.3 Phase 2 available):
    - Call scene planning service to generate shot list with camera metadata
    - Generate scene dependencies and narrative flow
    - Generate consistency groupings for entities
    - Generate entity descriptions with consistency requirements
    - If VideoDirectorGPT not available: Document fallback behavior (planning optional)
  - Implement image-to-video motion prompt enhancement:
    - Enhance motion descriptions with camera movement, motion intensity, frame rate considerations
    - Generate negative prompts for unwanted motion
  - Implement storyboard JSON processing:
    - Load and parse `storyboard_metadata.json` from Story 8.3/8.4
    - **Load unified narrative document** from `unified_narrative_path` in storyboard metadata (required for narrative context from Story 8.4)
    - **Load and validate start/end frame images** from `start_frame_path` and `end_frame_path` in each clip (generated by Story 8.3, used for image-to-video in Story 9.2)
    - Extract clip information: motion_description, camera_movement, shot_size, perspective, lens_type, start/end frame paths
    - **Pass unified narrative document as context** to two-agent enhancement loop for each clip (building upon Story 8.4)
    - **Pass start/end frame paths as visual context** to enhancement loop (for image-to-video generation in Story 9.2)
    - Process each clip's motion description through two-agent enhancement loop with narrative and visual context:
      - Agent 1 (Video Director) uses narrative context (emotional arc, visual progression, scene connections from Story 8.4) to enhance motion prompts
      - Agent 1 (Video Director) references start/end frames to ensure motion prompts align with visual context
      - Agent 2 (Prompt Engineer) validates narrative coherence in enhanced prompts (checks alignment with unified narrative)
      - Agent 2 (Prompt Engineer) validates visual consistency with start/end frames
    - Generate enhanced motion prompts per clip that maintain story coherence and visual consistency
    - **Preserve start/end frame paths** in output JSON for use in Story 9.2 video generation
    - **Preserve unified narrative document reference** in output JSON for context continuity
    - Create per-clip trace directories and files
    - Generate summary JSON with all enhanced motion prompts, frame paths, and narrative references
  - Implement trace file saving:
    - Save all prompt versions to numbered files
    - Save VideoDirectorGPT plan JSON (if available)
    - Save prompt_trace_summary.json with scores, iterations, timestamps
  - Create `VideoPromptEnhancementResult` class:
    - original_prompt, final_prompt, iterations, final_score, total_iterations
    - videodirectorgpt_plan (Optional[Dict]), motion_prompt (Optional[str])
  - Create `StoryboardMotionEnhancementResult` class:
    - storyboard_path: Path to original storyboard JSON
    - unified_narrative_path: Path to unified narrative document (from Story 8.4)
    - clips: List[Dict] with enhanced motion prompts per clip
    - clip_results: List[VideoPromptEnhancementResult] for each clip
    - clip_frame_paths: Dict mapping clip numbers to start/end frame paths (for Story 9.2)
    - summary: Dict with overall statistics
  - Unit tests: Two-agent iteration loop, prompt enhancement logic, scoring calculation (6 dimensions), convergence detection, VideoDirectorGPT integration (if available), image-to-video motion prompt enhancement, storyboard JSON parsing, per-clip motion prompt enhancement with narrative context

- Task 2: Integrate Prompt Scoring Guide guidelines (AC: #2)
  - Integrate Prompt Scoring Guide best practices into Agent 1 system prompt:
    - Structure like one-sentence screenplay (subject → action → setting → style → mood)
    - Use film terminology for clarity
    - Limit to one scene or action per prompt
    - Use visual language (describe what camera sees)
    - Specify camera framing, depth of field, action beats, lighting, palette
  - Update Agent 2 scoring criteria to validate Prompt Scoring Guide compliance
  - Unit tests: Verify enhanced prompts include camera/motion cues, follow screenplay structure, use film terminology

- Task 3: Create CLI tool for video prompt enhancement (AC: #1, #3, #4)
  - Create `backend/enhance_video_prompt.py` CLI script
  - Implement argument parsing (argparse or click):
    - Input: video prompt file path or "-" for stdin (mutually exclusive with --storyboard)
    - `--storyboard PATH` (optional, path to storyboard_metadata.json from Story 8.3)
    - `--video-mode` (default: True, enable video-specific enhancement)
    - `--image-to-video` (optional flag, enable image-to-video motion prompt mode)
    - `--max-iterations N` (default: 3, range: 1-5)
    - `--threshold F` (default: 85.0, score threshold to stop at)
    - `--output-dir DIR` (default: output/video_prompt_traces/)
    - `--creative-model M` (default: gpt-4-turbo)
    - `--critique-model M` (default: gpt-4-turbo)
    - `--verbose` flag
  - Implement input validation:
    - Either prompt file/stdin OR --storyboard must be provided (not both)
    - Validate storyboard JSON structure if --storyboard provided
    - Validate prompt file exists if file path provided
  - Implement prompt file loading (file or stdin)
  - Implement storyboard JSON loading:
    - Load and parse storyboard_metadata.json
    - Validate JSON structure (clips array, motion_description per clip, unified_narrative_path, start_frame_path, end_frame_path, etc.)
    - **Load unified narrative document** from `unified_narrative_path` (required - building upon Story 8.4)
    - **Load and validate start/end frame images** from paths in each clip (required for image-to-video generation in Story 9.2)
    - Extract clip information for processing (motion descriptions, camera metadata, frame paths)
    - Validate unified narrative document exists and is accessible
    - Validate start/end frame images exist and are readable
  - Implement output directory creation with timestamp
  - Integrate video prompt enhancement service
  - Implement trace file saving (for single prompt mode):
    - Save all prompt versions to numbered files (00_original_prompt.txt, 01_agent1_iteration_1.txt, etc.)
    - Save VideoDirectorGPT plan JSON (06_videodirectorgpt_plan.json) if available
    - Save prompt_trace_summary.json with metadata
  - Implement trace file saving (for storyboard mode):
    - Create per-clip subdirectories: `clip_001/`, `clip_002/`, etc.
    - Save per-clip trace files in each subdirectory (00_original_motion.txt, 01_agent1_iteration_1.txt, etc.)
    - Save enhanced motion prompts: `clip_001_enhanced_motion_prompt.txt`, etc.
    - Save `storyboard_enhanced_motion_prompts.json` with:
      - All clips' enhanced prompts and scores
      - **Start/end frame paths** for each clip (preserved from storyboard, for use in Story 9.2)
      - **Unified narrative document reference** (preserved from storyboard, for context continuity)
      - Original storyboard metadata reference
  - Implement console output formatting:
    - For single prompt mode: Enhancement results with scores and iteration history, final enhanced prompt, score breakdown (all 6 dimensions), file paths
    - For storyboard mode: List of clips processed, enhanced motion prompt per clip, score breakdown per clip, **start/end frame paths** for each clip (for use in video generation), file paths for manual viewing
  - Unit tests: Argument parsing, stdin input handling, file I/O, error handling, storyboard JSON parsing, validation logic
  - Integration test: End-to-end CLI run with sample prompt, verify trace files created, verify console output format
  - Integration test: End-to-end CLI run with storyboard JSON, verify per-clip enhanced motion prompts created, verify trace files per clip, verify summary JSON created, verify start/end frame paths preserved, verify unified narrative document reference preserved

- Task 4: Documentation and testing (All ACs)
  - Update `backend/requirements.txt` with any new dependencies (if needed)
  - Create README or usage documentation for CLI tool
  - Add integration tests to `backend/tests/test_video_prompt_enhancement.py`
  - Verify error handling for API failures, invalid inputs, missing files, VideoDirectorGPT planning failures, missing unified narrative documents, missing start/end frame images
  - Performance test: Verify video prompt enhancement completes within &lt;60 seconds for 2-iteration enhancement (target)
  - Document VideoDirectorGPT planning integration (if available) vs fallback behavior
    </tasks>
  </story>

  <acceptanceCriteria>
1. **Given** I have a basic video prompt (or motion prompt for image-to-video)
   **When** I run `python enhance_video_prompt.py prompt.txt --video-mode`
   **Then** the CLI tool:
   - Uses Agent 1 (Video Director/Creative Director) to enhance prompt with camera framing/movement, action beats, timing cues, lighting, color palette, motion intensity, temporal continuity hints
   - Uses Agent 2 (Prompt Engineer) to critique and score on 6 dimensions (0-100 each): Completeness, Specificity, Professionalism, Cinematography, Temporal Coherence, Brand Alignment
   - Iterates between agents (max 3 rounds) until score threshold met or convergence detected
   - Applies VideoDirectorGPT-style enhancements (if Story 7.3 Phase 2 available): Shot list, scene dependencies, narrative flow, consistency groupings
   - Saves all prompt versions to `output/video_prompt_traces/{timestamp}/` with numbered files
   - Saves `prompt_trace_summary.json` with scores, iterations, timestamps, VideoDirectorGPT plan
   - Prints enhancement results to console with scores and iteration history
   - Supports stdin input: `echo "prompt" | python enhance_video_prompt.py -`
   - Supports custom output directories: `--output-dir ./my_traces`
   - Supports iteration control: `--max-iterations 5 --threshold 90`
   - For image-to-video mode: Enhances motion prompts with camera movement, motion intensity, frame rate considerations, negative prompts

2. **Given** prompt enhancement follows Prompt Scoring Guide guidelines
   **Then** enhanced prompts:
   - Structure like one-sentence screenplay (subject → action → setting → style → mood)
   - Use film terminology for clarity
   - Limit to one scene or action per prompt
   - Use visual language (describe what camera sees)
   - Specify camera framing, depth of field, action beats, lighting, palette

3. **Given** I use image-to-video mode (`--image-to-video`)
   **Then** the CLI tool:
   - Enhances motion descriptions with:
     - Camera movement (pan, tilt, dolly, static, tracking)
     - Motion intensity (subtle, moderate, dynamic)
     - Frame rate considerations
     - Negative prompts for unwanted motion (e.g., "jerky, flicker, inconsistent")
   - Saves motion prompt separately in trace files

4. **Given** I have a storyboard JSON file from Story 8.3/8.4 (`storyboard_metadata.json`) that includes:
   - Start/end frame images (`clip_XXX_start.png`, `clip_XXX_end.png`) generated by Story 8.3
   - Unified narrative document (`unified_narrative.md` and `unified_narrative.json`) generated by Story 8.4
   - Motion descriptions, camera metadata, and frame paths for each clip
   **When** I run `python enhance_video_prompt.py --storyboard storyboard_metadata.json`
   **Then** the CLI tool:
   - Loads the storyboard JSON file and parses clip information
   - **Loads the unified narrative document** (from `unified_narrative_path` in storyboard metadata) to use as narrative context for maintaining story coherence across clips
   - **Loads start/end frame images** (from `start_frame_path` and `end_frame_path` in each clip) to use as visual context for image-to-video generation
   - For each clip in the storyboard:
     - Extracts the clip's `motion_description` as the base motion prompt
     - Extracts camera metadata: `camera_movement`, `shot_size`, `perspective`, `lens_type`
     - **Extracts start/end frame paths** (`start_frame_path`, `end_frame_path`) for use in subsequent video generation (Story 9.2)
     - **Uses unified narrative document as context** to ensure motion prompts:
       - Maintain story coherence with overall narrative (from Story 8.4)
       - Follow the emotional arc progression from the narrative (Anticipation → Recognition → Connection → Aspiration)
       - Apply consistent visual progression strategy (abstract → product-focused → lifestyle)
       - Create smooth narrative transitions between clips
       - Align with scene connections and product reveal strategy from the narrative
     - Enhances the motion prompt using two-agent feedback loop:
       - Agent 1 (Video Director): Enhances motion description with:
         - Detailed camera movement specifications
         - Motion intensity and style
         - Frame rate considerations
         - Temporal continuity hints
         - **Narrative context integration** (emotional arc, visual progression, scene connections from unified narrative)
         - **Visual context awareness** (references to start/end frames for image-to-video generation)
       - Agent 2 (Prompt Engineer): Critiques and scores the enhanced motion prompt, validating:
         - Narrative coherence with unified narrative document
         - Alignment with emotional arc and visual progression
         - Consistency with start/end frame visual context
       - Iterates (max 3 rounds) until score threshold met or convergence detected
     - Generates negative prompts for unwanted motion (e.g., "jerky, flicker, inconsistent")
   - Saves enhanced motion prompts per clip to `output/video_prompt_traces/{timestamp}/`:
     - `clip_001_enhanced_motion_prompt.txt` - Enhanced motion prompt for clip 1
     - `clip_002_enhanced_motion_prompt.txt` - Enhanced motion prompt for clip 2
     - `clip_003_enhanced_motion_prompt.txt` - Enhanced motion prompt for clip 3
     - etc. (one file per clip)
   - Saves per-clip trace files in subdirectories:
     - `clip_001/` - Trace files for clip 1 enhancement (00_original_motion.txt, 01_agent1_iteration_1.txt, etc.)
     - `clip_002/` - Trace files for clip 2 enhancement
     - etc.
   - Saves `storyboard_enhanced_motion_prompts.json` with:
     - Original storyboard metadata reference
     - Enhanced motion prompts for each clip
     - Scores and iteration history per clip
     - **Start/end frame paths preserved** for use in Story 9.2 video generation
     - **Unified narrative document reference** preserved for context
     - Summary of all clips processed
   - Prints enhancement results to console:
     - List of clips processed
     - Enhanced motion prompt for each clip
     - Score breakdown per clip (all 6 dimensions)
     - **Start/end frame paths** for each clip (for use in video generation)
     - File paths for manual viewing
   - Supports custom output directories: `--output-dir ./my_traces`
   - **Outputs are ready for Story 9.2**: Enhanced motion prompts, start/end frame paths, and narrative context are preserved for video generation workflow
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/PRD.md" title="Product Requirements Document">
        <section>Hero-Frame Iteration Plan &amp; Timeline (Section 23.0)</section>
        <snippet>CLI MVP strategy for rapid validation of video generation workflows before building UI components. Epic 9 establishes CLI-based rapid iteration tools for video prompt enhancement and video generation. FR-039, FR-040 implemented as CLI tools (`enhance_video_prompt.py`, `generate_videos.py`).</snippet>
      </doc>
      <doc path="docs/architecture.md" title="Architecture Document">
        <section>Project Structure, Service Organization</section>
        <snippet>Backend services organized in `app/services/pipeline/`. CLI tools are standalone Python scripts in `backend/` directory. Configuration management via `app/core/config.py` for API keys. Output directory structure: `backend/output/` for organizing outputs.</snippet>
      </doc>
      <doc path="docs/sprint-artifacts/tech-spec-epic-9.md" title="Epic 9 Technical Specification">
        <section>Detailed Design, Services and Modules, Workflows</section>
        <snippet>Complete technical specification for Epic 9 CLI tools. Defines `video_prompt_enhancement.py` service with storyboard processing mode, `enhance_video_prompt.py` CLI tool, VideoDirectorGPT integration, trace file structure, acceptance criteria, and test strategy. Storyboard processing integrates with `storyboard_service.py` (Story 8.3) and uses unified narrative documents from Story 8.4 to maintain narrative coherence across video clips.</snippet>
      </doc>
      <doc path="docs/epics.md" title="Epic Breakdown">
        <section>Epic 9: CLI MVP - Video Generation Feedback Loops, Story 9.1</section>
        <snippet>Original story acceptance criteria. Prerequisites: Story 7.3 Phase 1 (Two-Agent Prompt Enhancement), Story 8.1 (Image Prompt Enhancement pattern), Story 8.3 (Storyboard Creation), Story 8.4 (Unified Narrative Generation). Technical notes on reusing `prompt_enhancement.py` service pattern. Storyboard processing mode with unified narrative context integration.</snippet>
      </doc>
      <doc path="docs/Prompt_Scoring_and_Optimization_Guide.md" title="Prompt Scoring and Optimization Guide">
        <section>Best Practices, Video Prompt Structure</section>
        <snippet>Best practices for video prompt structure (one-sentence screenplay style). Film terminology guidelines. Visual language requirements. Camera framing, depth of field, action beats, lighting, palette specifications. Temporal coherence considerations for video prompts. Motion description best practices for image-to-video generation.</snippet>
      </doc>
      <doc path="docs/sprint-artifacts/8-3-storyboard-creation-cli.md" title="Story 8.3 Storyboard Creation CLI">
        <section>Storyboard Service, Output Structure</section>
        <snippet>Storyboard service generates start/end frame images (`clip_XXX_start.png`, `clip_XXX_end.png`) for each clip. Storyboard metadata JSON includes `start_frame_path` and `end_frame_path` for each clip. These frame paths are used for image-to-video generation in Story 9.2.</snippet>
      </doc>
      <doc path="docs/sprint-artifacts/8-4-unified-narrative-generation.md" title="Story 8.4 Unified Narrative Generation">
        <section>Unified Narrative Document, Narrative Context Integration</section>
        <snippet>Storyboard system generates unified narrative document (markdown + JSON) that describes overall ad story, emotional arc, visual progression, and scene connections. This document is essential for maintaining narrative coherence in video generation. Narrative path available via `unified_narrative_path` in `storyboard_metadata.json`. Epic 8.4 AC #4 explicitly states Epic 9 should use narrative document for coherence - this is now a required part of Story 9.1 storyboard processing.</snippet>
      </doc>
    </docs>
    <code>
      <artifact path="backend/app/services/pipeline/image_prompt_enhancement.py" kind="service" symbol="enhance_prompt_iterative" lines="127-219" reason="Two-agent pattern implementation to adapt for video-specific enhancement. Agent system prompts (CINEMATOGRAPHER_SYSTEM_PROMPT, PROMPT_ENGINEER_SYSTEM_PROMPT) need adaptation for video (add motion, temporal coherence, cinematography focus). Scoring mechanism uses 5 dimensions; video needs 6 dimensions (add Temporal Coherence).">
        <interface name="ImagePromptEnhancementResult" kind="class" signature="class ImagePromptEnhancementResult: original_prompt: str, final_prompt: str, iterations: List[Dict], final_score: Dict[str, float], total_iterations: int" path="backend/app/services/pipeline/image_prompt_enhancement.py" />
        <interface name="enhance_prompt_iterative" kind="async function" signature="async def enhance_prompt_iterative(user_prompt: str, max_iterations: int = 3, score_threshold: float = 85.0, creative_model: str = 'gpt-4-turbo', critique_model: str = 'gpt-4-turbo', trace_dir: Optional[Path] = None) -> ImagePromptEnhancementResult" path="backend/app/services/pipeline/image_prompt_enhancement.py" />
      </artifact>
      <artifact path="backend/app/services/pipeline/prompt_enhancement.py" kind="service" symbol="enhance_prompt_iterative" lines="94-192" reason="Base two-agent pattern from Story 7.3 Phase 1. Reuse iteration loop structure, convergence detection, trace file saving mechanism. Adapt Agent 1 system prompt for video-specific enhancement (motion, temporal coherence, cinematography focus).">
        <interface name="PromptEnhancementResult" kind="class" signature="class PromptEnhancementResult: original_prompt: str, final_prompt: str, iterations: List[Dict], final_score: Dict[str, float], total_iterations: int" path="backend/app/services/pipeline/prompt_enhancement.py" />
        <interface name="enhance_prompt_iterative" kind="async function" signature="async def enhance_prompt_iterative(user_prompt: str, max_iterations: int = 3, score_threshold: float = 85.0, creative_model: str = 'gpt-4-turbo', critique_model: str = 'gpt-4-turbo', trace_dir: Optional[Path] = None) -> PromptEnhancementResult" path="backend/app/services/pipeline/prompt_enhancement.py" />
      </artifact>
      <artifact path="backend/enhance_image_prompt.py" kind="cli" symbol="main" lines="85-204" reason="CLI tool pattern to follow for `enhance_video_prompt.py`. Argument parsing with argparse, stdin input handling (`-`), output directory management with timestamps, trace file organization, console output formatting, error handling. Supports custom output directories and iteration control.">
        <interface name="load_prompt" kind="function" signature="def load_prompt(input_source: str) -> str" path="backend/enhance_image_prompt.py" />
        <interface name="print_results" kind="function" signature="def print_results(result, output_dir: Path)" path="backend/enhance_image_prompt.py" />
        <interface name="main" kind="async function" signature="async def main()" path="backend/enhance_image_prompt.py" />
      </artifact>
      <artifact path="backend/app/services/pipeline/scene_planning.py" kind="service" symbol="plan_scenes" lines="32-80" reason="Scene planning service for VideoDirectorGPT integration (if Story 7.3 Phase 2 available). `plan_scenes()` function for framework-based scene planning. `create_basic_scene_plan_from_prompt()` for basic planning without LLM. Framework templates (PAS, BAB, AIDA). Adapt for VideoDirectorGPT-style planning if available, otherwise use basic planning or skip.">
        <interface name="plan_scenes" kind="function" signature="def plan_scenes(ad_spec: AdSpecification, target_duration: int = 15) -> ScenePlan" path="backend/app/services/pipeline/scene_planning.py" />
        <interface name="create_basic_scene_plan_from_prompt" kind="function" signature="def create_basic_scene_plan_from_prompt(prompt: str, target_duration: int, num_scenes: int) -> ScenePlan" path="backend/app/services/pipeline/scene_planning.py" />
      </artifact>
      <artifact path="backend/app/core/config.py" kind="config" symbol="settings" reason="Configuration management for API keys. Access OpenAI API key via `settings.OPENAI_API_KEY` for prompt enhancement. Follow existing pattern for environment variable loading.">
        <interface name="settings" kind="object" signature="Settings object with OPENAI_API_KEY, REPLICATE_API_TOKEN, etc." path="backend/app/core/config.py" />
      </artifact>
      <artifact path="backend/app/services/pipeline/storyboard_service.py" kind="service" symbol="create_storyboard" lines="54-552" reason="Storyboard service from Story 8.3. Generates storyboard with start/end frames. `StoryboardResult` class with clips list and metadata. `ClipStoryboard` class contains motion_description, camera_movement, shot_size, perspective, lens_type, start_frame_path, end_frame_path. Storyboard metadata JSON structure includes `unified_narrative_path` field pointing to unified narrative document. Each clip in metadata includes `start_frame_path` and `end_frame_path` for image-to-video generation.">
        <interface name="StoryboardResult" kind="class" signature="class StoryboardResult: clips: List[ClipStoryboard], metadata: Dict[str, Any], output_dir: str, timestamp: str" path="backend/app/services/pipeline/storyboard_service.py" />
        <interface name="ClipStoryboard" kind="class" signature="class ClipStoryboard: clip_number: int, start_frame_path: str, end_frame_path: str, motion_description: str, camera_movement: str, shot_size: str, perspective: str, lens_type: str" path="backend/app/services/pipeline/storyboard_service.py" />
        <interface name="create_storyboard" kind="async function" signature="async def create_storyboard(prompt: str, num_clips: int = 3, aspect_ratio: str = '16:9', reference_image_path: Optional[str] = None, output_dir: Optional[Path] = None) -> StoryboardResult" path="backend/app/services/pipeline/storyboard_service.py" />
      </artifact>
      <artifact path="backend/app/services/pipeline/storyboard_prompt_enhancement.py" kind="service" symbol="enhance_storyboard_prompts" lines="343-645" reason="Storyboard prompt enhancement service from Story 8.4. `enhance_storyboard_prompts()` function generates unified narrative document. `_generate_unified_narrative()` function creates narrative with emotional arc, visual progression, scene connections. Unified narrative saved as `unified_narrative.md` and `unified_narrative.json` in storyboard trace directory. Narrative path included in `storyboard_metadata.json` via `unified_narrative_path` field. Critical for Story 9.1: Narrative document provides context for motion prompt enhancement to maintain story coherence. Narrative JSON structure includes overall_story, emotional_arc, scene_connections, visual_progression, product_reveal_strategy, brand_narrative.">
        <interface name="StoryboardEnhancementResult" kind="class" signature="class StoryboardEnhancementResult: original_prompt: str, reference_image_path: str, extracted_visual_elements: Dict, scene_prompts: List[ScenePromptSet], unified_narrative_md: Optional[str], unified_narrative_json: Optional[Dict], unified_narrative_path: Optional[str]" path="backend/app/services/pipeline/storyboard_prompt_enhancement.py" />
        <interface name="enhance_storyboard_prompts" kind="async function" signature="async def enhance_storyboard_prompts(original_prompt: str, reference_image_path: str, num_scenes: int = 3, max_iterations: int = 3, score_threshold: float = 85.0, trace_dir: Optional[Path] = None) -> StoryboardEnhancementResult" path="backend/app/services/pipeline/storyboard_prompt_enhancement.py" />
        <interface name="_generate_unified_narrative" kind="async function" signature="async def _generate_unified_narrative(original_prompt: str, visual_elements: Dict[str, Any], num_scenes: int = 3, max_iterations: int = 3, score_threshold: float = 85.0, trace_dir: Optional[Path] = None) -> Dict[str, Any]" path="backend/app/services/pipeline/storyboard_prompt_enhancement.py" />
      </artifact>
    </code>
    <dependencies>
      <ecosystem name="python">
        <package name="openai" version=">=1.0.0" reason="OpenAI API client for prompt enhancement agents (GPT-4-turbo)" />
        <package name="pydantic" reason="Data validation and settings management" />
        <package name="asyncio" reason="Async/await pattern for API calls" />
        <package name="pathlib" reason="Path handling for file operations" />
        <package name="json" reason="JSON parsing for storyboard metadata and unified narrative documents" />
      </ecosystem>
      <ecosystem name="system">
        <package name="python" version="3.11+" reason="Python runtime requirement" />
      </ecosystem>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Service Location: New service file `app/services/pipeline/video_prompt_enhancement.py` follows existing service structure in `app/services/pipeline/`. Uses async/await pattern for API calls (OpenAI). Imports from `app.core.config` for settings. Imports from `app.services.pipeline.scene_planning` for VideoDirectorGPT planning (if available). Imports from `app.services.pipeline.image_prompt_enhancement` for base pattern adaptation. Imports from `app.services.pipeline.storyboard_service` for storyboard JSON structure reference.</constraint>
    <constraint>CLI Tool Location: New CLI tool `backend/enhance_video_prompt.py` is standalone script (not part of FastAPI app). Can be run independently: `python enhance_video_prompt.py prompt.txt` OR `python enhance_video_prompt.py --storyboard storyboard_metadata.json`. Uses relative imports to access services: `from app.services.pipeline.video_prompt_enhancement import ...`</constraint>
    <constraint>Input Modes: CLI supports two mutually exclusive input modes: (1) Single prompt mode: prompt file path or stdin ("-"), (2) Storyboard mode: --storyboard PATH to storyboard_metadata.json. Input validation must ensure only one mode is used.</constraint>
    <constraint>Storyboard Processing: When --storyboard is provided, service must load and parse storyboard JSON, extract clip information (motion_description, camera metadata, frame paths), **load unified narrative document** from `unified_narrative_path` field (required for narrative context from Story 8.4), **load and validate start/end frame images** from `start_frame_path` and `end_frame_path` in each clip (required for image-to-video generation in Story 9.2), process each clip's motion description through two-agent enhancement loop with narrative and visual context, generate enhanced motion prompts per clip that maintain story coherence and visual consistency, **preserve start/end frame paths** in output JSON for use in Story 9.2 video generation, **preserve unified narrative document reference** in output JSON for context continuity.</constraint>
    <constraint>Unified Narrative Integration: **CRITICAL** - When processing storyboards, MUST load unified narrative document from `unified_narrative_path` in storyboard metadata. Pass narrative context (emotional arc, visual progression, scene connections, product reveal strategy) to Agent 1 (Video Director) during enhancement. Agent 2 (Prompt Engineer) must validate narrative coherence in enhanced prompts. This ensures story coherence across all video clips and maintains the narrative foundation from Story 8.4.</constraint>
    <constraint>Start/End Frame Preservation: **CRITICAL** - When processing storyboards, MUST preserve `start_frame_path` and `end_frame_path` from each clip in the output JSON. These frame paths are generated by Story 8.3 and are required for image-to-video generation in Story 9.2. Frame paths must be validated to exist and be readable before processing.</constraint>
    <constraint>Output Directory Structure: Follow existing pattern: `backend/output/video_prompt_traces/{timestamp}/` for trace files. Timestamp format: `YYYYMMDD_HHMMSS` (e.g., `20250117_143022`). For single prompt mode: Numbered trace files: `00_original_prompt.txt`, `01_agent1_iteration_1.txt`, etc. VideoDirectorGPT plan JSON: `06_videodirectorgpt_plan.json` (if available). Summary metadata JSON: `prompt_trace_summary.json`. For storyboard mode: Per-clip subdirectories: `clip_001/`, `clip_002/`, etc. with trace files per clip. Enhanced motion prompts: `clip_001_enhanced_motion_prompt.txt`, etc. Summary JSON: `storyboard_enhanced_motion_prompts.json` with all clips' enhanced prompts, scores, **start/end frame paths**, and **unified narrative document reference**.</constraint>
    <constraint>VideoDirectorGPT Planning: Story 7.3 Phase 2 (LLM-Guided Multi-Scene Planning) is in backlog. If not available: Use basic scene planning from `scene_planning.py` as fallback (optional), or skip planning (tool still works without it). Document fallback behavior clearly.</constraint>
    <constraint>Scoring Dimensions: Image prompts use 5 dimensions; video prompts need 6 dimensions (add Temporal Coherence: 0-100, does it describe smooth, plausible motion?). Other dimensions: Completeness, Specificity, Professionalism, Cinematography, Brand Alignment.</constraint>
    <constraint>Prompt Scoring Guide Compliance: Enhanced prompts must follow Prompt Scoring Guide best practices: Structure like one-sentence screenplay (subject → action → setting → style → mood), use film terminology, limit to one scene per prompt, use visual language, specify camera framing/depth of field/action beats/lighting/palette.</constraint>
    <constraint>Configuration Management: Use `app/core/config.py` for API keys. Access OpenAI API key via `settings.OPENAI_API_KEY` (for prompt enhancement). Follow existing pattern for environment variable loading.</constraint>
    <constraint>Performance Target: Video prompt enhancement should complete within &lt;60 seconds for 2-iteration enhancement (target). For storyboard mode: target &lt;60 seconds per clip (for 3 clips: &lt;3 minutes total). Log timing information for each major operation (enhancement, scoring, planning).</constraint>
  </constraints>

  <interfaces>
    <interface name="VideoPromptEnhancementResult" kind="class" signature="class VideoPromptEnhancementResult: original_prompt: str, final_prompt: str, iterations: List[Dict], final_score: Dict[str, float], total_iterations: int, videodirectorgpt_plan: Optional[Dict], motion_prompt: Optional[str]" path="app/services/pipeline/video_prompt_enhancement.py" />
    <interface name="enhance_video_prompt_iterative" kind="async function" signature="async def enhance_video_prompt_iterative(user_prompt: str, max_iterations: int = 3, score_threshold: float = 85.0, creative_model: str = 'gpt-4-turbo', critique_model: str = 'gpt-4-turbo', trace_dir: Optional[Path] = None, video_mode: bool = True, image_to_video: bool = False) -> VideoPromptEnhancementResult" path="app/services/pipeline/video_prompt_enhancement.py" />
    <interface name="enhance_storyboard_motion_prompts" kind="async function" signature="async def enhance_storyboard_motion_prompts(storyboard_path: Path, max_iterations: int = 3, score_threshold: float = 85.0, creative_model: str = 'gpt-4-turbo', critique_model: str = 'gpt-4-turbo', trace_dir: Optional[Path] = None) -> StoryboardMotionEnhancementResult" path="app/services/pipeline/video_prompt_enhancement.py" />
    <interface name="StoryboardMotionEnhancementResult" kind="class" signature="class StoryboardMotionEnhancementResult: storyboard_path: Path, unified_narrative_path: Path, clips: List[Dict], clip_results: List[VideoPromptEnhancementResult], clip_frame_paths: Dict[int, Dict[str, str]], summary: Dict" path="app/services/pipeline/video_prompt_enhancement.py" />
    <interface name="OpenAI API" kind="REST API" signature="openai.ChatCompletion.create(model='gpt-4-turbo', messages=[system_prompt, user_prompt])" path="External API" />
    <interface name="Scene Planning Service" kind="function" signature="plan_scenes(ad_spec: AdSpecification, target_duration: int = 15) -> ScenePlan OR create_basic_scene_plan_from_prompt(prompt: str, target_duration: int, num_scenes: int) -> ScenePlan" path="app/services/pipeline/scene_planning.py" />
    <interface name="Storyboard JSON" kind="data format" signature="storyboard_metadata.json structure: {clips: [{clip_number, motion_description, camera_movement, shot_size, perspective, lens_type, start_frame_path, end_frame_path}], unified_narrative_path: str, ...}" path="output/storyboards/{timestamp}/storyboard_metadata.json" />
    <interface name="Unified Narrative Document" kind="data format" signature="unified_narrative.json structure: {overall_story: {narrative, framework, total_scenes, target_duration}, emotional_arc: {scene_1: {scene_type, emotional_state, visual_mood, product_visibility, narrative_purpose}, ...}, scene_connections: {...}, visual_progression: {...}, product_reveal_strategy: {...}, brand_narrative: {...}}" path="output/storyboards/{timestamp}/unified_narrative.json" />
  </interfaces>

  <tests>
    <standards>Use pytest framework (matches existing backend test structure). Test video prompt enhancement service: Two-agent iteration loop, prompt enhancement logic, scoring calculation (6 dimensions), convergence detection, VideoDirectorGPT integration (if available), image-to-video motion prompt enhancement, storyboard JSON parsing, per-clip motion prompt enhancement with narrative context. Test CLI argument parsing, stdin input handling, file I/O, error handling, storyboard JSON parsing, validation logic. Target: &gt;80% code coverage. Integration tests: End-to-end CLI execution with sample prompt, verify trace files created with correct structure (including VideoDirectorGPT plan if available), verify console output format. Integration tests: End-to-end CLI execution with storyboard JSON, verify unified narrative document loaded, verify start/end frame images validated, verify per-clip enhanced motion prompts created with narrative context, verify trace files per clip, verify summary JSON created with preserved frame paths and narrative reference. Performance tests: Measure latency, target &lt;60 seconds for 2-iteration enhancement, target &lt;60 seconds per clip for storyboard mode (for 3 clips: &lt;3 minutes total), log timing information for each major operation.</standards>
    <locations>Unit tests: `backend/tests/test_video_prompt_enhancement.py`. Integration tests: `backend/tests/integration/test_enhance_video_prompt_cli.py` (or similar). Test data: Sample prompts in `backend/tests/fixtures/` or inline test data. Sample storyboard JSON files in test fixtures.</locations>
    <ideas>
      <idea>Unit test: Two-agent iteration loop - verify Agent 1 enhances prompt, Agent 2 critiques and scores, iteration continues until threshold met or max iterations reached, convergence detection works correctly.</idea>
      <idea>Unit test: Prompt enhancement logic - verify enhanced prompts include camera framing/movement, action beats, timing cues, lighting, color palette, motion intensity, temporal continuity hints (for video mode).</idea>
      <idea>Unit test: Scoring calculation (6 dimensions) - verify scoring on Completeness, Specificity, Professionalism, Cinematography, Temporal Coherence, Brand Alignment (all 0-100), overall score calculation (weighted average).</idea>
      <idea>Unit test: Convergence detection - verify early stopping when score threshold met, verify early stopping when no improvement detected, verify max iterations respected.</idea>
      <idea>Unit test: VideoDirectorGPT integration (if available) - verify VideoDirectorGPT planning called when available, verify shot list with camera metadata generated, verify scene dependencies and narrative flow generated, verify consistency groupings for entities generated, verify fallback behavior when VideoDirectorGPT not available.</idea>
      <idea>Unit test: Image-to-video motion prompt enhancement - verify motion descriptions enhanced with camera movement (pan, tilt, dolly, static, tracking), motion intensity (subtle, moderate, dynamic), frame rate considerations, negative prompts for unwanted motion generated.</idea>
      <idea>Unit test: Prompt Scoring Guide compliance - verify enhanced prompts follow one-sentence screenplay structure (subject → action → setting → style → mood), use film terminology, limit to one scene per prompt, use visual language, specify camera framing/depth of field/action beats/lighting/palette.</idea>
      <idea>Unit test: Storyboard JSON parsing - test loading storyboard_metadata.json, test extracting clip information (motion_description, camera_movement, start_frame_path, end_frame_path), test loading unified_narrative_path field, test validation of JSON structure, test error handling for invalid/missing storyboard files.</idea>
      <idea>Unit test: Unified narrative document loading - test loading unified_narrative.json from path in storyboard metadata, test parsing narrative JSON structure (overall_story, emotional_arc, scene_connections, visual_progression, product_reveal_strategy, brand_narrative), test error handling for missing/invalid narrative documents.</idea>
      <idea>Unit test: Start/end frame validation - test loading and validating start_frame_path and end_frame_path from each clip, test error handling for missing frame images, test error handling for unreadable frame images.</idea>
      <idea>Unit test: Per-clip motion prompt enhancement with narrative context - test processing each clip's motion description through two-agent loop with unified narrative context, test Agent 1 uses narrative context (emotional arc, visual progression, scene connections) to enhance motion prompts, test Agent 1 references start/end frames for visual context awareness, test Agent 2 validates narrative coherence in enhanced prompts, test Agent 2 validates visual consistency with start/end frames, test generating enhanced motion prompts per clip that maintain story coherence and visual consistency.</idea>
      <idea>Unit test: Frame path preservation - test preserving start_frame_path and end_frame_path from each clip in output JSON, test preserving unified_narrative_path in output JSON, test output JSON structure includes all required fields for Story 9.2 video generation.</idea>
      <idea>Unit test: CLI argument parsing - verify all arguments parsed correctly (input file/stdin, --storyboard, --video-mode, --image-to-video, --max-iterations, --threshold, --output-dir, --creative-model, --critique-model, --verbose), verify default values applied, verify validation (max-iterations range 1-5, threshold range), verify mutually exclusive input modes (prompt file/stdin vs --storyboard).</idea>
      <idea>Unit test: Stdin input handling - verify stdin input (`-`) works correctly, verify prompt loaded from stdin, verify error handling for empty stdin.</idea>
      <idea>Unit test: File I/O - verify trace files created with correct structure (numbered files, VideoDirectorGPT plan JSON if available, prompt_trace_summary.json), verify output directory creation with timestamp, verify file content correctness. For storyboard mode: verify per-clip trace directories created, verify per-clip trace files saved, verify enhanced motion prompts saved, verify storyboard_enhanced_motion_prompts.json created with preserved frame paths and narrative reference.</idea>
      <idea>Unit test: Error handling - verify API failures handled gracefully (OpenAI API errors, rate limits, timeouts), verify invalid inputs handled (empty prompts, invalid file paths), verify missing files handled, verify VideoDirectorGPT planning failures handled (fallback behavior), verify missing unified narrative documents handled, verify missing start/end frame images handled.</idea>
      <idea>Integration test: End-to-end CLI run - run CLI with sample video prompt file, verify trace files created with correct structure (including VideoDirectorGPT plan if available), verify console output format (enhancement results, scores, iteration history, final prompt, score breakdown, file paths), verify all 6 dimensions scored correctly.</idea>
      <idea>Integration test: Storyboard mode end-to-end - run CLI with storyboard JSON file, verify storyboard JSON loaded and parsed correctly, verify unified narrative document loaded from unified_narrative_path, verify start/end frame images validated, verify per-clip enhanced motion prompts created (clip_001_enhanced_motion_prompt.txt, etc.) with narrative context integration, verify per-clip trace directories created (clip_001/, clip_002/, etc.), verify storyboard_enhanced_motion_prompts.json created with all clips' enhanced prompts, scores, **start/end frame paths preserved**, **unified narrative document reference preserved**, verify console output shows list of clips processed with scores per clip and frame paths.</idea>
      <idea>Performance test: Measure latency - run video prompt enhancement with 2 iterations, verify completes within &lt;60 seconds (target), log timing information for each major operation (enhancement, scoring, planning), identify bottlenecks. For storyboard mode: measure total time for all clips, verify completes within &lt;60 seconds per clip (for 3 clips: &lt;3 minutes total), log timing per clip.</idea>
    </ideas>
  </tests>
</story-context>
